{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d52291-87d5-4b37-9c70-d7348fcf33b9",
   "metadata": {},
   "source": [
    "# BPSD: Score-Audio Synchronization\n",
    "\n",
    "- Align notes from score (in csv format) with audio recordings, using measure positions as anchors.\n",
    "- requirements: Sync Toolbox code (https://github.com/meinardmueller/synctoolbox)\n",
    "\n",
    "[1] Sebastian Ewert, Meinard M체ller, and Peter Grosche. \"High resolution audio synchronization using chroma onset features.\" 2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2009.  \n",
    "\n",
    "[2] Thomas Pr채tzlich, Jonathan Driedger, and Meinard M체ller. \"Memory-restricted multiscale dynamic time warping.\" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.\n",
    "\n",
    "[3] Meinard M체ller et al. \"Sync Toolbox: A Python package for efficient, robust, and accurate music synchronization.\" Journal of Open Source Software 6.64 (2021): 3434.\n",
    "\n",
    "Johannes Zeitler (johannes.zeitler@audiolabs-erlangen.de), 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0fd5b06-e43e-41fc-bf40-50d6001d1b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(\"path_to_synctoolbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c687c6-a69d-4cb7-a6a5-b7930ec28dd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading some modules and defining some constants used later\n",
    "import IPython.display as ipd\n",
    "from libfmp.b import list_to_pitch_activations, plot_chromagram, plot_signal, plot_matrix, \\\n",
    "                     sonify_pitch_activations_with_signal, read_csv\n",
    "import libfmp.c2\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.interpolate\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from synctoolbox.dtw.mrmsdtw import sync_via_mrmsdtw, __check_anchor_pairs, __split_features, __diagonal_warping_path, sync_via_mrmsdtw_with_anchors\n",
    "from synctoolbox.dtw.utils import compute_optimal_chroma_shift, shift_chroma_vectors, make_path_strictly_monotonic\n",
    "from synctoolbox.feature.csv_tools import read_csv_to_df, df_to_pitch_features, df_to_pitch_onset_features\n",
    "from synctoolbox.feature.chroma import pitch_to_chroma, quantize_chroma, quantized_chroma_to_CENS\n",
    "from synctoolbox.feature.dlnco import pitch_onset_features_to_DLNCO, __visualize_LN_features\n",
    "from synctoolbox.feature.pitch import audio_to_pitch_features\n",
    "from synctoolbox.feature.pitch_onset import audio_to_pitch_onset_features\n",
    "from synctoolbox.feature.utils import estimate_tuning\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "from midi_utils import save_midi\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import groupby\n",
    "from copy import deepcopy\n",
    "\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13bd7734-4200-465a-afff-ccb897a0ddd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Fs = 22050\n",
    "feature_rate = 50\n",
    "step_weights = np.array([1.5, 1.5, 2.0])\n",
    "threshold_rec = 10 ** 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bf87c-8427-45f9-88b5-9a09052dabca",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229cba65-404f-4dbd-bd72-24acb5b5fe52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_arr(arr, val, tol=1e-4):\n",
    "    \n",
    "    minIdx = np.argmin(np.abs(arr - val))\n",
    "    \n",
    "    if np.abs(arr[minIdx]-val) <= tol:\n",
    "        return True, minIdx\n",
    "    else:\n",
    "        return False, None\n",
    "    \n",
    "    #return np.min(np.abs(df-val)) <= tol\n",
    "\n",
    "def isint(val, tol=1e-10):\n",
    "    return np.abs(np.mod(val, 1)) <= tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df0d641-be21-4762-a755-3c08089af69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def writeMP3(f, sr, x, normalized=False):\n",
    "    \"\"\"numpy array to MP3\"\"\"\n",
    "    channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
    "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
    "        y = np.int16(x * 2 ** 15)\n",
    "    else:\n",
    "        y = np.int16(x)\n",
    "    song = AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
    "    song.export(f, format=\"mp3\", bitrate=\"192k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2486c6b-db1d-4abc-a6f8-6db62885cbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_csv_to_df(csv_filepath: str = '',\n",
    "                   csv_delimiter: str = ';') -> pd.DataFrame:\n",
    "    \"\"\"Reads .csv file containing symbolic music into a pandas DataFrame.\n",
    "    Column names are normalized to be lower case.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        Filepath to the .csv file.\n",
    "\n",
    "    csv_delimiter : str\n",
    "        Delimiter of the .csv file (default: ';')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.Dataframe\n",
    "        Annotations in pandas Dataframe format.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filepath_or_buffer=csv_filepath,\n",
    "                     delimiter=csv_delimiter)#, dtype=\"str\")#, index_col=0)\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    if 'pitch' in df.columns:\n",
    "        df['pitch'] = df['pitch'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060b99eb-7d81-4f2e-8cc5-fa472eb78c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dynamics_to_velocity(df, velMap={\"ppp\":20, \"pp\":39, \"p\":61, \"mp\":71, \"mf\":84, \"f\":98, \"fp\":98, \"ff\": 113, \"fff\":127, \"sf\":113}):\n",
    "    velocities = []\n",
    "    for i, row in df.iterrows():\n",
    "        velocities.append(velMap[row[\"dynamics\"]])\n",
    "    \n",
    "    df[\"velocity\"] = velocities\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22058dd0-2a71-4413-8a03-7e9e3365587b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sync_via_mrmsdtw_with_anchors(f_chroma1: np.ndarray,\n",
    "                                  f_chroma2: np.ndarray,\n",
    "                                  f_onset1: np.ndarray = None,\n",
    "                                  f_onset2: np.ndarray = None,\n",
    "                                  input_feature_rate: int = 50,\n",
    "                                  step_sizes: np.ndarray = np.array([[1, 0], [0, 1], [1, 1]], np.int32),\n",
    "                                  step_weights: np.ndarray = np.array([1.0, 1.0, 1.0], np.float64),\n",
    "                                  threshold_rec: int = 10000,\n",
    "                                  win_len_smooth: np.ndarray = np.array([201, 101, 21, 1]),\n",
    "                                  downsamp_smooth: np.ndarray = np.array([50, 25, 5, 1]),\n",
    "                                  verbose: bool = False,\n",
    "                                  dtw_implementation: str = 'synctoolbox',\n",
    "                                  normalize_chroma: bool = True,\n",
    "                                  chroma_norm_ord: int = 2,\n",
    "                                  chroma_norm_threshold: float = 0.001,\n",
    "                                  visualization_title: str = \"MrMsDTW result\",\n",
    "                                  anchor_pairs: List[Tuple] = None,\n",
    "                                  linear_inp_idx: List[int] = [],\n",
    "                                  alpha=0.5) -> np.ndarray:\n",
    "    \"\"\"Compute memory-restricted multi-scale DTW (MrMsDTW) using chroma and (optionally) onset features.\n",
    "        MrMsDTW is performed on multiple levels that get progressively finer, with rectangular constraint\n",
    "        regions defined by the alignment found on the previous, coarser level.\n",
    "        If onset features are provided, these are used on the finest level in addition to chroma\n",
    "        to provide higher synchronization accuracy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f_chroma1 : np.ndarray [shape=(12, N)]\n",
    "            Chroma feature matrix of the first sequence\n",
    "\n",
    "        f_chroma2 : np.ndarray [shape=(12, M)]\n",
    "            Chroma feature matrix of the second sequence\n",
    "\n",
    "        f_onset1 : np.ndarray [shape=(L, N)]\n",
    "            Onset feature matrix of the first sequence (optional, default: None)\n",
    "\n",
    "        f_onset2 : np.ndarray [shape=(L, M)]\n",
    "            Onset feature matrix of the second sequence (optional, default: None)\n",
    "\n",
    "        input_feature_rate: int\n",
    "            Input feature rate of the chroma features (default: 50)\n",
    "\n",
    "        step_sizes: np.ndarray\n",
    "            DTW step sizes (default: np.array([[1, 0], [0, 1], [1, 1]]))\n",
    "\n",
    "        step_weights: np.ndarray\n",
    "            DTW step weights (np.array([1.0, 1.0, 1.0]))\n",
    "\n",
    "        threshold_rec: int\n",
    "            Defines the maximum area that is spanned by the rectangle of two\n",
    "            consecutive elements in the alignment (default: 10000)\n",
    "\n",
    "        win_len_smooth : np.ndarray\n",
    "            Window lengths for chroma feature smoothing (default: np.array([201, 101, 21, 1]))\n",
    "\n",
    "        downsamp_smooth : np.ndarray\n",
    "            Downsampling factors (default: np.array([50, 25, 5, 1]))\n",
    "\n",
    "        verbose : bool\n",
    "            Set `True` for visualization (default: False)\n",
    "\n",
    "        dtw_implementation : str\n",
    "            DTW implementation, librosa or synctoolbox (default: synctoolbox)\n",
    "\n",
    "        normalize_chroma : bool\n",
    "            Set `True` to normalize input chroma features after each downsampling\n",
    "            and smoothing operation.\n",
    "\n",
    "        chroma_norm_ord: int\n",
    "            Order of chroma normalization, relevant if ``normalize_chroma`` is True.\n",
    "            (default: 2)\n",
    "\n",
    "        chroma_norm_threshold: float\n",
    "            If the norm falls below threshold for a feature vector, then the\n",
    "            normalized feature vector is set to be the unit vector. Relevant, if\n",
    "            ``normalize_chroma`` is True (default: 0.001)\n",
    "\n",
    "        visualization_title : str\n",
    "            Title for the visualization plots. Only relevant if 'verbose' is True\n",
    "            (default: \"MrMsDTW result\")\n",
    "\n",
    "        anchor_pairs: List[Tuple]\n",
    "            Anchor pairs given in seconds. Note that\n",
    "            * (0, 0) and (<audio-len1>, <audio-len2>) are not allowed.\n",
    "            * Anchors must be monotonously increasing.\n",
    "\n",
    "        linear_inp_idx: List[int]\n",
    "            List of the indices of intervals created by anchor pairs, for which\n",
    "            MrMsDTW shouldn't be run, e.g., if the interval only involves silence.\n",
    "\n",
    "            0        ap1        ap2        ap3\n",
    "            |         |          |          |\n",
    "            |  idx0   |   idx1   |  idx2    |  idx3 OR idx-1\n",
    "            |         |          |          |\n",
    "\n",
    "            Note that index -1 corresponds to the last interval, which begins with\n",
    "            the last anchor pair until the end of the audio files.\n",
    "\n",
    "        alpha: float\n",
    "            Coefficient for the Chroma cost matrix in the finest scale of the MrMsDTW algorithm.\n",
    "            C = alpha * C_Chroma + (1 - alpha) * C_act  (default: 0.5)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wp : np.ndarray [shape=(2, T)]\n",
    "            Resulting warping path which indicates synchronized indices.\n",
    "    \"\"\"\n",
    "        \n",
    "    wp_cur_list = []    \n",
    "    \n",
    "    if anchor_pairs is None:\n",
    "        wp = sync_via_mrmsdtw(f_chroma1=f_chroma1,\n",
    "                              f_chroma2=f_chroma2,\n",
    "                              f_onset1=f_onset1,\n",
    "                              f_onset2=f_onset2,\n",
    "                              input_feature_rate=input_feature_rate,\n",
    "                              step_sizes=step_sizes,\n",
    "                              step_weights=step_weights,\n",
    "                              threshold_rec=threshold_rec,\n",
    "                              win_len_smooth=win_len_smooth,\n",
    "                              downsamp_smooth=downsamp_smooth,\n",
    "                              verbose=verbose,\n",
    "                              dtw_implementation=dtw_implementation,\n",
    "                              normalize_chroma=normalize_chroma,\n",
    "                              chroma_norm_ord=chroma_norm_ord,\n",
    "                              chroma_norm_threshold=chroma_norm_threshold,\n",
    "                              visualization_title=visualization_title,\n",
    "                              alpha=alpha)\n",
    "    else:\n",
    "        wp = None\n",
    "\n",
    "        if verbose:\n",
    "            print('Anchor points are given!')\n",
    "\n",
    "\n",
    "        # Add ending as the anchor point\n",
    "        if (anchor_pairs[-1][0] < f_chroma1.shape[1]/input_feature_rate - 1/input_feature_rate) or (anchor_pairs[-1][1] < f_chroma2.shape[1]/input_feature_rate - 1/input_feature_rate):\n",
    "            anchor_pairs.append((f_chroma1.shape[1]/input_feature_rate, f_chroma2.shape[1]/input_feature_rate))\n",
    "            \n",
    "        prev_a1 = 0\n",
    "        prev_a2 = 0\n",
    "        \n",
    "        flag_quit = False\n",
    "\n",
    "        for idx, anchor_pair in enumerate(anchor_pairs):\n",
    "            cur_a1, cur_a2 = anchor_pair\n",
    "            \n",
    "            if cur_a1 == 0:\n",
    "                wp_cur = np.concatenate([-np.ones(int(cur_a2*input_feature_rate))[None,:],\n",
    "                                         np.arange(int(cur_a2*input_feature_rate))[None,:]],\n",
    "                                        axis=0)\n",
    "                wp_cur_list.append(wp_cur)\n",
    "            elif cur_a2 == 0:\n",
    "                wp_cur = np.concatenate([np.arange(int(cur_a1*input_feature_rate))[None,:],\n",
    "                                        -np.ones(int(cur_a1*input_feature_rate))[None,:]],\n",
    "                                        axis=0)\n",
    "                wp_cur_list.append(wp_cur)\n",
    "                \n",
    "            elif  (prev_a1 - f_chroma1.shape[1]/input_feature_rate) >= -1/input_feature_rate:\n",
    "                indices_2 = np.arange( f_chroma2.shape[1] - int(prev_a2*input_feature_rate))\n",
    "                \n",
    "                \n",
    "                wp_cur = np.concatenate([ np.ones_like(indices_2),\n",
    "                                          #int(prev_a1*input_feature_rate) + 1 + np.zeros_like(indices_2),\n",
    "                                          indices_2], axis=0)\n",
    "                wp_cur_list.append(wp_cur)\n",
    "                \n",
    "                \n",
    "                flag_quit=True\n",
    "                \n",
    "                \n",
    "            elif  (prev_a2 - f_chroma2.shape[1]/input_feature_rate) >= -1/input_feature_rate:\n",
    "                indices_1 = np.arange( f_chroma1.shape[1] - int(prev_a1*input_feature_rate))\n",
    "                \n",
    "                wp_cur = np.concatenate([indices_1,                     \n",
    "                                        np.ones_like(indices_1),\n",
    "                                          ], axis=0)\n",
    "                wp_cur_list.append(wp_cur)\n",
    "                \n",
    "                \n",
    "                flag_quit=True\n",
    "                \n",
    "            \n",
    "                \n",
    "            else:\n",
    "\n",
    "                # Split the features\n",
    "                f_chroma1_split, f_onset1_split, f_chroma2_split, f_onset2_split = __split_features(f_chroma1,\n",
    "                                                                                                    f_onset1,\n",
    "                                                                                                    f_chroma2,\n",
    "                                                                                                    f_onset2,\n",
    "                                                                                                    cur_a1,\n",
    "                                                                                                    cur_a2,\n",
    "                                                                                                    prev_a1,\n",
    "                                                                                                    prev_a2,\n",
    "                                                                                                    input_feature_rate)\n",
    "\n",
    "                if idx in linear_inp_idx or idx == len(anchor_pairs) - 1 and -1 in linear_inp_idx:\n",
    "                    # Generate a diagonal warping path, if the algorithm is not supposed to executed.\n",
    "                    # A typical scenario is the silence breaks which are enclosed by two anchor points.\n",
    "                    if verbose:\n",
    "                        print('A diagonal warping path is generated for the interval \\n\\t Feature sequence 1: %.2f - %.2f'\n",
    "                              '\\n\\t Feature sequence 2: %.2f - %.2f\\n' % (prev_a1, cur_a1, prev_a2, cur_a2))\n",
    "                    wp_cur = __diagonal_warping_path(f_chroma1_split, f_chroma2_split)\n",
    "                    wp_cur_list.append(wp_cur)\n",
    "\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        if cur_a1 != -1 and cur_a2 != -1:\n",
    "                            print('MrMsDTW is applied for the interval \\n\\t Feature sequence 1: %.2f - %.2f'\n",
    "                                  '\\n\\t Feature sequence 2: %.2f - %.2f\\n' % (prev_a1, cur_a1, prev_a2, cur_a2))\n",
    "                        else:\n",
    "                            print('MrMsDTW is applied for the interval \\n\\t Feature sequence 1: %.2f - end'\n",
    "                                  '\\n\\t Feature sequence 2: %.2f - end\\n' % (prev_a1, prev_a2))\n",
    "                    wp_cur = sync_via_mrmsdtw(f_chroma1=f_chroma1_split,\n",
    "                                              f_chroma2=f_chroma2_split,\n",
    "                                              f_onset1=f_onset1_split,\n",
    "                                              f_onset2=f_onset2_split,\n",
    "                                              input_feature_rate=input_feature_rate,\n",
    "                                              step_sizes=step_sizes,\n",
    "                                              step_weights=step_weights,\n",
    "                                              threshold_rec=threshold_rec,\n",
    "                                              win_len_smooth=win_len_smooth,\n",
    "                                              downsamp_smooth=downsamp_smooth,\n",
    "                                              verbose=verbose,\n",
    "                                              dtw_implementation=dtw_implementation,\n",
    "                                              normalize_chroma=normalize_chroma,\n",
    "                                              chroma_norm_ord=chroma_norm_ord,\n",
    "                                              chroma_norm_threshold=chroma_norm_threshold,\n",
    "                                              alpha=alpha)\n",
    "                    wp_cur_list.append(wp_cur)\n",
    "\n",
    "            if wp is None:\n",
    "                wp = np.array(wp_cur, copy=True)\n",
    "\n",
    "            # Concatenate warping paths\n",
    "            else:\n",
    "                wp = np.concatenate([wp, wp_cur + wp[:, -1].reshape(2, 1) + 1], axis=1)\n",
    "                \n",
    "            prev_a1 = cur_a1\n",
    "            prev_a2 = cur_a2\n",
    "            \n",
    "            if flag_quit: \n",
    "                break\n",
    "\n",
    "        anchor_pairs.pop()\n",
    "\n",
    "    return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4584c2f-952c-4f65-849c-11d5d54f81f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pitch_onset_matrix_to_DLNCO(pitch_onset_matrix : np.ndarray,\n",
    "                                  feature_sequence_length: int,\n",
    "                                  feature_rate: int = 50,\n",
    "                                  midi_min: int = 21,\n",
    "                                  midi_max: int = 108,\n",
    "                                  log_compression_gamma: float = 10000.0,\n",
    "                                  chroma_norm_ord: int = 2,\n",
    "                                  LN_maxfilterlength_seconds: float = 0.8,\n",
    "                                  LN_maxfilterthresh: float = 0.1,\n",
    "                                  DLNCO_filtercoef: np.ndarray = np.sqrt(1 / np.arange(1, 11)),\n",
    "                                  visualize=False) -> np.ndarray:\n",
    "    \"\"\"Computes decaying locally adaptive normalized chroma onset (DLNCO) features from\n",
    "    a dictionary of peaks obtained e.g. by ``audio_to_pitch_onset_features``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_peaks : dict\n",
    "        A dictionary of onset peaks\n",
    "\n",
    "            * Each key corresponds to the midi pitch number\n",
    "\n",
    "            * Each value f_peaks[midi_pitch] is an array of doubles of size 2xN:\n",
    "\n",
    "                + First row give the positions of the peaks in milliseconds.\n",
    "\n",
    "                + Second row contains the corresponding magnitudes of the peaks.\n",
    "\n",
    "    feature_sequence_length : int\n",
    "        Desired length of the resulting feature sequence. This should be at least as long as the\n",
    "        position of the last peak in ``f_peaks``, but can be longer.\n",
    "\n",
    "    feature_rate : int\n",
    "        Desired features per second in the output representation\n",
    "\n",
    "    midi_min : int\n",
    "        Minimum MIDI pitch index (default: 21)\n",
    "\n",
    "    midi_max : int\n",
    "        Maximum MIDI pitch index (default: 108)\n",
    "\n",
    "    log_compression_gamma : float\n",
    "        Gamma factor of the log compression applied to peak magnitudes.\n",
    "        \n",
    "    chroma_norm_ord : int\n",
    "        Order of the norm used for chroma onset vectors.\n",
    "\n",
    "    LN_maxfilterlength_seconds : float\n",
    "        Length of the maximum filter applied for determining local norm of chroma onsets in seconds.\n",
    "\n",
    "    LN_maxfilterthresh : float\n",
    "        Minimum threshold for normalizing chroma onsets using local norm.\n",
    "\n",
    "    DLNCO_filtercoef : np.ndarray\n",
    "        Sequence of decay coefficients applied on normalized chroma onsets.\n",
    "\n",
    "    visualize : bool\n",
    "        Set `True` to visualize chroma onset features (Default: False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f_DLNCO : np.array [shape=(d_dlnco, N_dlnco)]\n",
    "        Decaying Locally adaptively Normalized Chroma Onset features\n",
    "    \"\"\"\n",
    "    \n",
    "    f_CO = pitch_to_chroma(pitch_onset_matrix).T\n",
    "\n",
    "    # No two ways to normalize F_CO: simply columnwise (f_N) or via local\n",
    "    # normalizing curve (f_LN)\n",
    "    f_N = np.zeros(feature_sequence_length)\n",
    "\n",
    "    for k in range(feature_sequence_length):\n",
    "        f_N[k] = np.linalg.norm(f_CO[k, :], chroma_norm_ord)\n",
    "\n",
    "    f_LN = np.array(f_N, copy=True)\n",
    "    f_left = np.array(f_N, copy=True)\n",
    "    f_right = np.array(f_N, copy=True)\n",
    "    LN_maxfilterlength_frames = int(LN_maxfilterlength_seconds * feature_rate)\n",
    "    if LN_maxfilterlength_frames % 2 == 1:\n",
    "        LN_maxfilterlength_frames -= 1\n",
    "    shift = int(np.floor((LN_maxfilterlength_frames) / 2))\n",
    "\n",
    "    # TODO improve with scipy.ndimage.maximum_filter\n",
    "    for s in range(shift):\n",
    "        f_left = np.roll(f_left, 1, axis=0)\n",
    "        f_left[0] = 0\n",
    "        f_right = np.roll(f_right, -1, axis=0)\n",
    "        f_right[-1] = 0\n",
    "        f_LN = np.max([f_left, f_LN, f_right], axis=0)\n",
    "\n",
    "    f_LN = np.maximum(f_LN, LN_maxfilterthresh)\n",
    "\n",
    "    # Compute f_NC0 (normalizing f_C0 using f_N)\n",
    "    # f_NCO = np.zeros((feature_sequence_length, 12))\n",
    "\n",
    "    # Compute f_LNC0 (normalizing f_C0 using f_LN)\n",
    "    f_LNCO = np.zeros((feature_sequence_length, 12))\n",
    "    for k in range(feature_sequence_length):\n",
    "        # f_NCO[k, :] = f_CO[k, :] / (f_N[k]) #+ eps)\n",
    "        f_LNCO[k, :] = f_CO[k, :] / f_LN[k]\n",
    "\n",
    "    # Compute f_DLNCO\n",
    "    f_DLNCO = np.zeros((feature_sequence_length, 12))\n",
    "\n",
    "    num_coef = DLNCO_filtercoef.size\n",
    "    for p_idx in range(12):\n",
    "        v_shift = np.array(f_LNCO[:, p_idx], copy=True)\n",
    "        v_help = np.zeros((feature_sequence_length, num_coef))\n",
    "\n",
    "        for n in range(num_coef):\n",
    "            v_help[:, n] = DLNCO_filtercoef[n] * v_shift\n",
    "            v_shift = np.roll(v_shift, 1)\n",
    "            v_shift[0] = 0\n",
    "\n",
    "        f_DLNCO[:, p_idx] = np.max(v_help, axis=1)\n",
    "\n",
    "    # visualization\n",
    "    if visualize:\n",
    "        plot_chromagram(X=f_CO.T, title='CO', colorbar=True, Fs=feature_rate, colorbar_aspect=50, figsize=(9, 3))\n",
    "        __visualize_LN_features(f_N, f_LN, feature_sequence_length, feature_rate)\n",
    "        plot_chromagram(X=f_LNCO.T, title='LNCO', colorbar=True, Fs=feature_rate, colorbar_aspect=50, figsize=(9, 3))\n",
    "        plot_chromagram(X=f_DLNCO.T, title='DLNCO', colorbar=True, Fs=feature_rate, colorbar_aspect=50, figsize=(9, 3))\n",
    "\n",
    "    f_DLNCO = f_DLNCO.T\n",
    "\n",
    "    return f_DLNCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529fe22a-20d0-4749-bfc3-5cf43ff49302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_annotation(df_annotation, feature_rate, visualize=True):\n",
    "    if \"velocity\" not in df_annotation.keys():\n",
    "        df_annotation[\"velocity\"] = 64\n",
    "    f_pitch = df_to_pitch_features(df_annotation, feature_rate=feature_rate)\n",
    "    f_chroma = pitch_to_chroma(f_pitch=f_pitch)\n",
    "    f_chroma_quantized = quantize_chroma(f_chroma=f_chroma)\n",
    "    if visualize:\n",
    "        plot_chromagram(f_chroma_quantized, title='Quantized chroma features - Annotation', Fs=feature_rate, figsize=(9, 3))\n",
    "    f_pitch_onset = df_to_pitch_onset_features(df_annotation)\n",
    "    f_DLNCO = pitch_onset_features_to_DLNCO(f_peaks=f_pitch_onset,\n",
    "                                            feature_rate=feature_rate,\n",
    "                                            feature_sequence_length=f_chroma_quantized.shape[1],\n",
    "                                            visualize=visualize)\n",
    "    \n",
    "    return f_chroma_quantized, f_DLNCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbede0c-7daa-4d22-8c24-b275c126261b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce44406-8471-4723-a1ad-a03c561d8ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a277205-bf74-4293-a9c1-69995c6d31e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_dir = os.path.join(\"../\", \"1_Audio\")\n",
    "csv_in_dir_measures = os.path.join(\"../\", \"2_Annotations\", \"ann_score_note\")\n",
    "anchor_dir = os.path.join(\"../\", \"2_Annotations\", \"ann_audio_startEnd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4afa02-5eac-4aee-a0d5-9ad8b5e3cea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tuned transcription features, measure-wise anchor points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4538d6a-9d7f-4e7f-9b69-210240466154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "note_out_dir = os.path.join(\"../\", \"2_Annotations\", \"ann_audio_note\")\n",
    "onset_frames_dir = \"path_to_transcription_results\"\n",
    "measure_in_dir = os.path.join(\"../\", \"2_Annotations\", \"ann_audio_measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01563a81-3057-4b29-b16e-d98eaf7220b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pieces = list(set([\"_\".join(f.split(\"_\")[:-1]) for f in os.listdir(audio_dir) if \".wav\" in f]))\n",
    "pieces.sort()\n",
    "\n",
    "performers = list(set([(f.split(\".\")[0].split(\"_\")[-1]) for f in os.listdir(audio_dir) if \".wav\" in f]))\n",
    "performers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9613d7-da5f-4337-b894-a015f9f53240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for performer in performers:\n",
    "    \n",
    "    for piece in tqdm(pieces):\n",
    "        if not \"026\" in piece:continue\n",
    "        print(\"Processing %s_%s\"%(piece, performer))\n",
    "        audio, _ = librosa.load(os.path.join(audio_dir, \"%s_%s.wav\"%(piece, performer)), Fs)\n",
    "\n",
    "        df_annotation = read_csv_to_df(os.path.join(csv_in_dir_measures, piece+\".csv\"), csv_delimiter=';')\n",
    "        quarterNoteOffset=[]\n",
    "        beat_fac = 1\n",
    "        curr_meas=0.\n",
    "        meas_offset=0\n",
    "        \n",
    "        measuresIn = read_csv_to_df(os.path.join(measure_in_dir, \"%s_%s.csv\"%(piece, performer)), csv_delimiter=\";\")\n",
    "        measuresIn.sort_values(by=\"measure\", inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        measuresIn = pd.concat([pd.DataFrame({\"time\":[0, len(audio)/Fs], \"measure\":[0, int(max(measuresIn.measure))+1]}), measuresIn])\n",
    "        \n",
    "        measuresIn.sort_values(by=\"measure\", inplace=True)\n",
    "        \n",
    "        meas_start_time = []        \n",
    "        beat_fac_list = []\n",
    "        quarterNoteOffset_in_measure = []\n",
    "        \n",
    "        for i, row in df_annotation.iterrows():           \n",
    "\n",
    "            # if new measure starts\n",
    "            if np.floor(row.start_meas) > curr_meas:\n",
    "                curr_meas = np.floor(row.start_meas)\n",
    "                meas_offset += beat_fac*4\n",
    "                \n",
    "            meas_start_time.append(measuresIn.time[measuresIn.measure==curr_meas].item())\n",
    "\n",
    "            beat_fac = int(row.timesig.split(\"/\")[0])/int(row.timesig.split(\"/\")[1])\n",
    "            quarterNoteOffset.append( meas_offset+(row.start_meas-curr_meas)*beat_fac*4)\n",
    "            quarterNoteOffset_in_measure.append((row.start_meas-curr_meas)*beat_fac*4)\n",
    "            \n",
    "            beat_fac_list.append(beat_fac)\n",
    "\n",
    "        df_annotation[\"quarternoteoffset\"] = quarterNoteOffset\n",
    "        df_annotation[\"quarternoteoffset\"] -= min(df_annotation[\"quarternoteoffset\"])\n",
    "        \n",
    "        df_annotation[\"measstarttime\"] = meas_start_time\n",
    "        df_annotation[\"quarternoteoffsetinmeasure\"] = quarterNoteOffset_in_measure\n",
    "        #########################################################################\n",
    "        \n",
    "        measure_duration = []\n",
    "        for _, row in df_annotation.iterrows():\n",
    "            rowMeas = int(row.start_meas)\n",
    "            measure_duration.append( measuresIn.time[measuresIn.measure==(rowMeas+1)].item() - measuresIn.time[measuresIn.measure==rowMeas].item())\n",
    "            \n",
    "        df_annotation[\"beatfac\"] = beat_fac_list\n",
    "        df_annotation[\"secondspermeasure\"] = measure_duration\n",
    "        df_annotation[\"secondsperbeat\"] = df_annotation[\"secondspermeasure\"] / 4 / df_annotation[\"beatfac\"]\n",
    "        \n",
    "        df_annotation[\"start\"] = df_annotation[\"measstarttime\"] + df_annotation[\"quarternoteoffsetinmeasure\"]*df_annotation[\"secondsperbeat\"]#df_annotation[\"offset\"]*secondsperquarter#df_annotation[\"secondsperquarter\"]  \n",
    "        df_annotation[\"duration\"] = df_annotation[\"duration_quarterlength\"]*df_annotation[\"secondsperbeat\"] - 0.01 #df_annotation[\"secondsperquarter\"]\n",
    "                \n",
    "        df_annotation.duration[df_annotation.articulation == \"staccato\"] /= 2  \n",
    "\n",
    "        df_annotation[\"end\"] = df_annotation[\"start\"] + df_annotation[\"duration\"]\n",
    "\n",
    "        df_annotation[\"instrument\"] = [\"piano\" for _ in df_annotation.iterrows()]\n",
    "        \n",
    "\n",
    "        featuresIn = np.load(os.path.join(onset_frames_dir, \"%s_%s.npz\"%(piece, performer)))\n",
    "\n",
    "        onsetsIn = featuresIn[\"onset_pred\"].T\n",
    "        framesIn = featuresIn[\"frame_pred\"].T\n",
    "\n",
    "        frame_rate_in = featuresIn[\"sample_rate\"] / featuresIn[\"hop_length\"]\n",
    "\n",
    "\n",
    "        idx_interp = np.floor(np.arange(0, len(audio)/Fs, 1/feature_rate)*frame_rate_in).astype(int)\n",
    "\n",
    "        onsetsIn_res = onsetsIn[:,idx_interp]\n",
    "        framesIn_res = framesIn[:,idx_interp]\n",
    "\n",
    "        f_frames_transcription = np.zeros((128, framesIn_res.shape[1]))\n",
    "        f_frames_transcription[21:109,:] = framesIn_res\n",
    "\n",
    "        f_onsets_transcription = np.zeros((128, onsetsIn_res.shape[1]))\n",
    "        f_onsets_transcription[21:109,:] = onsetsIn_res\n",
    "\n",
    "\n",
    "        f_chroma_transcriptions = pitch_to_chroma(f_pitch=f_frames_transcription)\n",
    "        f_chroma_quantized_transcription = quantize_chroma(f_chroma = f_chroma_transcriptions)\n",
    "\n",
    "\n",
    "        f_DLNCO_transcription = pitch_onset_matrix_to_DLNCO(pitch_onset_matrix = f_onsets_transcription, feature_rate=feature_rate, \n",
    "                                                              feature_sequence_length=f_chroma_quantized_transcription.shape[1], visualize=False)\n",
    "\n",
    "        f_chroma_quantized_annotation, f_DLNCO_annotation = get_features_from_annotation(df_annotation, feature_rate, visualize=False)\n",
    "        \n",
    "        \n",
    "        if f_chroma_quantized_annotation.shape[1] < f_chroma_quantized_transcription.shape[1]:\n",
    "            f_chroma_quantized_annotation = np.concatenate([f_chroma_quantized_annotation, np.zeros((12, f_chroma_quantized_transcription.shape[1] - f_chroma_quantized_annotation.shape[1]))], axis=-1)\n",
    "            f_DLNCO_annotation = np.concatenate([f_DLNCO_annotation, np.zeros((12, f_DLNCO_transcription.shape[1] - f_DLNCO_annotation.shape[1]))], axis=-1)\n",
    "       \n",
    "        start_pairs = [[start_m, start_s] for start_m, start_s in zip(df_annotation.start_meas, df_annotation.start)]\n",
    "        start_pairs.sort()\n",
    "        start_pairs_unique = np.array([pair for pair,_ in groupby(start_pairs)])\n",
    "        for _, row in measuresIn.iterrows():\n",
    "            isInArr, idx = in_arr(start_pairs_unique[:,0], row.measure)\n",
    "            if not isInArr:\n",
    "                start_pairs_unique = np.concatenate([start_pairs_unique, np.array([row.measure, row.time])[None,:]], axis=0)\n",
    "        start_pairs_unique.sort(axis=0)        \n",
    "\n",
    "        meas_to_time_annot = scipy.interpolate.interp1d(start_pairs_unique[:,0], start_pairs_unique[:,1], \n",
    "                                                  kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "        anchor_pairs = [(row.time, meas_to_time_annot(row.measure).item()) for _,row in measuresIn.iterrows()][1:-1]\n",
    "        \n",
    "        anchor_pairs[-1] = (max(measuresIn.iloc[:-1].time), max(df_annotation.end))\n",
    "        \n",
    "\n",
    "        wp_transcription_weakMonotonic = sync_via_mrmsdtw_with_anchors(f_chroma1=f_chroma_quantized_transcription, \n",
    "                              f_onset1=f_DLNCO_transcription, \n",
    "                              f_chroma2=f_chroma_quantized_annotation, \n",
    "                              f_onset2=f_DLNCO_annotation, \n",
    "                              input_feature_rate=feature_rate, \n",
    "                              step_weights=step_weights, \n",
    "                              threshold_rec=threshold_rec, \n",
    "                              verbose=False,\n",
    "                              anchor_pairs=anchor_pairs) \n",
    "    \n",
    "       \n",
    "        wp_transcription = make_path_strictly_monotonic(deepcopy(wp_transcription_weakMonotonic))\n",
    "        df_annotation_warped_transcription = df_annotation.copy(deep=True)\n",
    "        df_annotation_warped_transcription[\"end\"] = df_annotation_warped_transcription[\"start\"] + df_annotation_warped_transcription[\"duration\"]\n",
    "        df_annotation_warped_transcription[['start', 'end']] = scipy.interpolate.interp1d(wp_transcription[1] / feature_rate, \n",
    "                                   wp_transcription[0] / feature_rate, kind='linear', bounds_error=False, fill_value=\"extrapolate\")(df_annotation[['start', 'end']])\n",
    "        \n",
    "        df_annotation_warped_transcription.end[df_annotation_warped_transcription.end > measuresIn.iloc[-2].time] = measuresIn.iloc[-2].time\n",
    "        \n",
    "        df_annotation_warped_transcription[\"duration\"] = df_annotation_warped_transcription[\"end\"] - df_annotation_warped_transcription[\"start\"]\n",
    "       \n",
    "\n",
    "        df_annotation_warped_transcription.sort_values(by=\"start\", inplace=True)\n",
    "        \n",
    "\n",
    "        df_annotation_warped_transcription.to_csv(os.path.join(note_out_dir, piece+\"_\"+performer+\".csv\"), sep=\";\", index=False,\n",
    "                                         columns=[\"start\", \"end\", \n",
    "                                                  \"start_meas\", \"end_meas\",\n",
    "                                                  \"pitch\", \"pitchname\",\n",
    "                                                  \"timesig\", \"articulation\"],\n",
    "                                         float_format=\"%07.3f\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822a23c-4bbf-4d8e-b9ea-9646f2def61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb177b1-001f-4aac-aa00-d410d717a239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b873748-3a03-4921-a5e0-cd7c694ed694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a689ed81-d631-43da-b8e5-ae3087e60da8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-Tuned transcription features, anchor points for start/end only\n",
    "\n",
    "... for evaluation of synchronization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd9b8928-ede4-4628-93f4-3f8d0b1a27a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "note_out_dir = os.path.join(\"../\", \"4_misc\", \"ann_audio_note_noAnchor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d41a9607-30f2-40e4-ab85-cb1054c2e243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pieces = [f.split(\".\")[0] for f in os.listdir(os.path.join(\"../\", \"2_Annotations\", \"ann_score_chord\")) if \".csv\" in f]\n",
    "pieces.sort()\n",
    "\n",
    "performers = list(set([f.split(\".\")[0].split(\"_\")[-1] for f in os.listdir(os.path.join(\"../\", \"2_Annotations\", \"ann_audio_measure\"))]))\n",
    "performers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79eb72d-0f74-4c70-81c2-a65ad35f4f49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for performer in performers:\n",
    "    \n",
    "    for piece in tqdm(pieces):\n",
    "        if not \"026\" in piece: continue\n",
    "        print(\"Processing %s_%s\"%(piece, performer))\n",
    "        audio, _ = librosa.load(os.path.join(audio_dir, \"%s_%s.wav\"%(piece, performer)), Fs)\n",
    "        \n",
    "        df_annotation = read_csv_to_df(os.path.join(csv_in_dir_measures, piece+\".csv\"), csv_delimiter=';')\n",
    "\n",
    "        quarterNoteOffset=[]\n",
    "        beat_fac = 1\n",
    "        curr_meas=0.\n",
    "        meas_offset=0\n",
    "        \n",
    "        measuresIn = read_csv_to_df(os.path.join(\"../\", \"2_Annotations\", \"ann_audio_measure\", \"%s_%s.csv\"%(piece, performer)), csv_delimiter=\";\")\n",
    "        measuresIn.sort_values(by=\"measure\", inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        measuresIn = pd.concat([pd.DataFrame({\"time\":[0, len(audio)/Fs], \"measure\":[0, int(max(measuresIn.measure))+1]}), measuresIn])\n",
    "        \n",
    "        measuresIn.sort_values(by=\"measure\", inplace=True)\n",
    "        \n",
    "        meas_start_time = []        \n",
    "        beat_fac_list = []\n",
    "        quarterNoteOffset_in_measure = []\n",
    "        \n",
    "        for i, row in df_annotation.iterrows():           \n",
    "\n",
    "            # if new measure starts\n",
    "            if np.floor(row.start_meas) > curr_meas:\n",
    "                curr_meas = np.floor(row.start_meas)\n",
    "                meas_offset += beat_fac*4\n",
    "                \n",
    "            meas_start_time.append(measuresIn.time[measuresIn.measure==curr_meas].item())\n",
    "\n",
    "            beat_fac = int(row.timesig.split(\"/\")[0])/int(row.timesig.split(\"/\")[1])\n",
    "            quarterNoteOffset.append( meas_offset+(row.start_meas-curr_meas)*beat_fac*4)\n",
    "            quarterNoteOffset_in_measure.append((row.start_meas-curr_meas)*beat_fac*4)\n",
    "            \n",
    "            beat_fac_list.append(beat_fac)\n",
    "\n",
    "        df_annotation[\"quarternoteoffset\"] = quarterNoteOffset\n",
    "        df_annotation[\"quarternoteoffset\"] -= min(df_annotation[\"quarternoteoffset\"])\n",
    "        \n",
    "        df_annotation[\"measstarttime\"] = meas_start_time\n",
    "        df_annotation[\"quarternoteoffsetinmeasure\"] = quarterNoteOffset_in_measure\n",
    "        #########################################################################\n",
    "        \n",
    "        measure_duration = []\n",
    "        for _, row in df_annotation.iterrows():\n",
    "            rowMeas = int(row.start_meas)\n",
    "            measure_duration.append( measuresIn.time[measuresIn.measure==(rowMeas+1)].item() - measuresIn.time[measuresIn.measure==rowMeas].item())\n",
    "            \n",
    "        df_annotation[\"beatfac\"] = beat_fac_list\n",
    "        df_annotation[\"secondspermeasure\"] = measure_duration\n",
    "        df_annotation[\"secondsperbeat\"] = df_annotation[\"secondspermeasure\"] / 4 / df_annotation[\"beatfac\"]\n",
    "        \n",
    "        df_annotation[\"start\"] = df_annotation[\"measstarttime\"] + df_annotation[\"quarternoteoffsetinmeasure\"]*df_annotation[\"secondsperbeat\"] \n",
    "        df_annotation[\"duration\"] = df_annotation[\"duration_quarterlength\"]*df_annotation[\"secondsperbeat\"] - 0.01 \n",
    "        \n",
    "       \n",
    "        df_annotation.duration[df_annotation.articulation == \"staccato\"] /= 2  \n",
    "\n",
    "        df_annotation[\"end\"] = df_annotation[\"start\"] + df_annotation[\"duration\"]\n",
    "\n",
    "        df_annotation[\"instrument\"] = [\"piano\" for _ in df_annotation.iterrows()]\n",
    "        \n",
    "        featuresIn = np.load(os.path.join(onset_frames_dir, \"%s_%s.npz\"%(piece, performer)))\n",
    "\n",
    "        onsetsIn = featuresIn[\"onset_pred\"].T\n",
    "        framesIn = featuresIn[\"frame_pred\"].T\n",
    "\n",
    "        frame_rate_in = featuresIn[\"sample_rate\"] / featuresIn[\"hop_length\"]\n",
    "\n",
    "\n",
    "        idx_interp = np.floor(np.arange(0, len(audio)/Fs, 1/feature_rate)*frame_rate_in).astype(int)\n",
    "\n",
    "        onsetsIn_res = onsetsIn[:,idx_interp]\n",
    "        framesIn_res = framesIn[:,idx_interp]\n",
    "\n",
    "        f_frames_transcription = np.zeros((128, framesIn_res.shape[1]))\n",
    "        f_frames_transcription[21:109,:] = framesIn_res\n",
    "\n",
    "        f_onsets_transcription = np.zeros((128, onsetsIn_res.shape[1]))\n",
    "        f_onsets_transcription[21:109,:] = onsetsIn_res\n",
    "\n",
    "\n",
    "        f_chroma_transcriptions = pitch_to_chroma(f_pitch=f_frames_transcription)\n",
    "        f_chroma_quantized_transcription = quantize_chroma(f_chroma = f_chroma_transcriptions)\n",
    "\n",
    "\n",
    "        f_DLNCO_transcription = pitch_onset_matrix_to_DLNCO(pitch_onset_matrix = f_onsets_transcription, feature_rate=feature_rate, \n",
    "                                                              feature_sequence_length=f_chroma_quantized_transcription.shape[1], visualize=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        f_chroma_quantized_annotation, f_DLNCO_annotation = get_features_from_annotation(df_annotation, feature_rate, visualize=False)\n",
    "        \n",
    "        \n",
    "        if f_chroma_quantized_annotation.shape[1] < f_chroma_quantized_transcription.shape[1]:\n",
    "            f_chroma_quantized_annotation = np.concatenate([f_chroma_quantized_annotation, np.zeros((12, f_chroma_quantized_transcription.shape[1] - f_chroma_quantized_annotation.shape[1]))], axis=-1)\n",
    "            f_DLNCO_annotation = np.concatenate([f_DLNCO_annotation, np.zeros((12, f_DLNCO_transcription.shape[1] - f_DLNCO_annotation.shape[1]))], axis=-1)\n",
    "\n",
    "        \n",
    "        start_pairs = [[start_m, start_s] for start_m, start_s in zip(df_annotation.start_meas, df_annotation.start)]\n",
    "        start_pairs.sort()\n",
    "        start_pairs_unique = np.array([pair for pair,_ in groupby(start_pairs)])\n",
    "        for _, row in measuresIn.iterrows():\n",
    "            isInArr, idx = in_arr(start_pairs_unique[:,0], row.measure)\n",
    "            if not isInArr:\n",
    "                start_pairs_unique = np.concatenate([start_pairs_unique, np.array([row.measure, row.time])[None,:]], axis=0)\n",
    "        start_pairs_unique.sort(axis=0)        \n",
    "\n",
    "        meas_to_time_annot = scipy.interpolate.interp1d(start_pairs_unique[:,0], start_pairs_unique[:,1], \n",
    "                                                  kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "\n",
    "        anchor_pairs = [(row.time, meas_to_time_annot(row.measure).item()) for _,row in measuresIn.iterrows()][1:-1]\n",
    "        \n",
    "        anchor_pairs[-1] = (max(measuresIn.iloc[:-1].time), max(df_annotation.end))\n",
    "        \n",
    "\n",
    "\n",
    "        wp_transcription_weakMonotonic = sync_via_mrmsdtw_with_anchors(f_chroma1=f_chroma_quantized_transcription, \n",
    "                              f_onset1=f_DLNCO_transcription, \n",
    "                              f_chroma2=f_chroma_quantized_annotation, \n",
    "                              f_onset2=f_DLNCO_annotation, \n",
    "                              input_feature_rate=feature_rate, \n",
    "                              step_weights=step_weights, \n",
    "                              threshold_rec=threshold_rec, \n",
    "                              verbose=False,\n",
    "                              anchor_pairs=[anchor_pairs[0], anchor_pairs[-1]]) \n",
    "\n",
    "        wp_transcription = make_path_strictly_monotonic(deepcopy(wp_transcription_weakMonotonic))\n",
    "\n",
    "        df_annotation_warped_transcription = df_annotation.copy(deep=True)\n",
    "        df_annotation_warped_transcription[\"end\"] = df_annotation_warped_transcription[\"start\"] + df_annotation_warped_transcription[\"duration\"]\n",
    "        df_annotation_warped_transcription[['start', 'end']] = scipy.interpolate.interp1d(wp_transcription[1] / feature_rate, \n",
    "                                   wp_transcription[0] / feature_rate, kind='linear', bounds_error=False, fill_value=\"extrapolate\")(df_annotation[['start', 'end']])\n",
    "        \n",
    "        df_annotation_warped_transcription.end[df_annotation_warped_transcription.end > measuresIn.iloc[-2].time] = measuresIn.iloc[-2].time\n",
    "        \n",
    "        df_annotation_warped_transcription[\"duration\"] = df_annotation_warped_transcription[\"end\"] - df_annotation_warped_transcription[\"start\"]\n",
    "        \n",
    "\n",
    "        df_annotation_warped_transcription.sort_values(by=\"start\", inplace=True)\n",
    "        \n",
    "        df_annotation_warped_transcription.to_csv(os.path.join(note_out_dir, piece+\"_\"+performer+\".csv\"), sep=\";\", index=False,\n",
    "                                         columns=[\"start\", \"end\", \n",
    "                                                  \"start_meas\", \"end_meas\",\n",
    "                                                  \"pitch\", \"pitchname\",\n",
    "                                                  \"timesig\", \"articulation\"],\n",
    "                                         float_format=\"%07.3f\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
