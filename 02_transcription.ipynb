{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4a59db-53b5-4615-a088-0974f4d123d0",
   "metadata": {},
   "source": [
    "# BPSD: Transcription of Audio Recordings\n",
    "\n",
    "\n",
    "- Transcribe all audio files using the NoteEM transcription model from B. Maman and A. Bermano, \"Unaligned supervision for automatic music transcription in the wild.\", International Conference on Machine Learning, 2022\n",
    "- Requirement: transcription code and checkpoint from https://github.com/benadar293/benadar293.github.io/\n",
    "\n",
    "Ben Maman, Johannes Zeitler (johannes.zeitler@audiolabs-erlangen.de), 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb130d0-c2d9-4120-b0da-46e0e1940191",
   "metadata": {},
   "source": [
    "Provide path to onsets and frames code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6e3737-f20c-496c-bd20-fa32fc8c79ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "onsets_frames_dir = \"path_to_transcription_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f57035-c697-4785-85f5-71324e6d20dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(onsets_frames_dir)\n",
    "\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "from onsets_and_frames.mel import *\n",
    "from onsets_and_frames.midi_utils import *\n",
    "import soundfile\n",
    "import librosa\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb0e79c-895c-4835-997f-ab05246f9b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CURR_N_FFT = 2048\n",
    "CURR_WIN_LENGTH = 2048\n",
    "CURR_NORM = 'slaney'\n",
    "CURR_N_MELS = N_MELS\n",
    "CURR_MEL_FMIN = 30\n",
    "CURR_HOP_LENGTH = 512\n",
    "\n",
    "USE_INSTRUMENT_GROUP = False # use general instrument group rather than specific instrument\n",
    "\n",
    "MAX_TIME = 10 * 60 * SAMPLE_RATE # max segment length in samples (to prevent OOM), if longer - transcribe segment by segment\n",
    "\n",
    "instrument_restriction = [0]\n",
    "\n",
    "# Fine-tuned by Ben Maman on Beethoven Piano Sonatas\n",
    "ckpt = \"path_to_checkpoint\"\n",
    "instrument_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5fef6cf-ba6d-41d7-9204-e2769298aae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#album = 'A Night at The Opera'\n",
    "\n",
    "out_pth = os.path.join(onsets_frames_dir, \"transcriptions\", \"BPSD\")\n",
    "os.makedirs(out_pth, exist_ok=True)\n",
    "os.makedirs(os.path.join(out_pth, \"midi\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_pth, \"raw_features\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f8d4fd9-ad6a-43d1-ae19-659c94e316dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset_th = 0.5\n",
    "frame_th = 0.5\n",
    "\n",
    "##########################################################################################\n",
    "inactive = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2743b0cb-148d-4596-a6bb-9ef85baf3bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type <class 'onsets_and_frames.transcriber.OnsetsAndFramesMultiV12'>\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(ckpt).cuda()\n",
    "print('model type', type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6bb5376-2bf8-4a75-9902-b32baf02ce87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_dir = os.path.join(\"../\", \"1_Audio\")\n",
    "\n",
    "pieces = [f for f in os.listdir(audio_dir) if \".wav\" in f]\n",
    "pieces.sort()\n",
    "\n",
    "audio_pths = [os.path.join(audio_dir, piece) for piece in pieces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c51bb49b-2c1c-4aa1-a24f-b28539015ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b8af9-01d6-4bc2-ac0f-3b3a3ea01653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "melspectrogram = MelSpectrogram(CURR_N_MELS, SAMPLE_RATE, filter_length=CURR_N_FFT, win_length=CURR_WIN_LENGTH,\n",
    "                                         hop_length=CURR_HOP_LENGTH, mel_fmin=CURR_MEL_FMIN, mel_fmax=8000,\n",
    "                                         norm=CURR_NORM)\n",
    "melspectrogram.to(DEFAULT_DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "for pth in tqdm(audio_pths):\n",
    "    if verbose: print('pth', pth)\n",
    "    audio, sr = librosa.load(pth, sr=16000)\n",
    "    if verbose: print('sr', sr)\n",
    "    if verbose: print('audio min max', audio.min(), audio.max())\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "    audio = np.clip(audio, a_min=-1., a_max=1.)\n",
    "    audio = torch.from_numpy(audio).float().cuda().unsqueeze(0)\n",
    "\n",
    "    audio_inp_len = audio.shape[1]\n",
    "    if audio_inp_len > MAX_TIME: # transcribe in segments of length MAX_TIME:\n",
    "        n_segments = audio_inp_len // MAX_TIME\n",
    "        if n_segments * MAX_TIME < audio_inp_len:\n",
    "            n_segments += 1\n",
    "        if verbose: print('long audio, splitting to {} segments'.format(n_segments))\n",
    "        seg_len = MAX_TIME\n",
    "        onsets_preds = []\n",
    "        frame_preds = []\n",
    "        inst_onsets_preds = []\n",
    "        for i_s in range(n_segments):\n",
    "            if verbose: print('segment', i_s)\n",
    "            curr = audio[:, i_s * seg_len: (i_s + 1) * seg_len]\n",
    "            curr_mel = melspectrogram(curr).permute((0, 2, 1))\n",
    "            try:\n",
    "                curr_onset_pred, _, _, curr_frame_pred = model(curr_mel)\n",
    "            except:\n",
    "                curr_onset_pred, _, _, curr_frame_pred, _ = model(curr_mel)\n",
    "            if instrument_ckpt is not None:\n",
    "                try:\n",
    "                    curr_inst_onset_pred, _, _, _ = instrument_model(curr_mel)\n",
    "                except:\n",
    "                    curr_inst_onset_pred, _, _, _, _ = instrument_model(curr_mel)\n",
    "\n",
    "            onsets_preds.append(curr_onset_pred)\n",
    "            frame_preds.append(curr_frame_pred)\n",
    "            if instrument_ckpt is not None:\n",
    "                inst_onsets_preds.append(curr_inst_onset_pred)\n",
    "        pitch_onset_pred = torch.cat(onsets_preds, dim=1)\n",
    "        if verbose: print('pitch onset shape', pitch_onset_pred.shape)\n",
    "        if instrument_ckpt is not None:\n",
    "            inst_onset_pred = torch.cat(inst_onsets_preds, dim=1)\n",
    "        frame_pred = torch.cat(frame_preds, dim=1)\n",
    "    else:\n",
    "        curr_mel = melspectrogram(audio).permute((0, 2, 1))\n",
    "        try:\n",
    "            pitch_onset_pred, offset_pred, activation_pred, frame_pred = model(curr_mel)\n",
    "        except:\n",
    "            pitch_onset_pred, offset_pred, activation_pred, frame_pred, vel_pred = model(curr_mel)\n",
    "        if instrument_ckpt is not None:\n",
    "            try:\n",
    "                inst_onset_pred, _, _, _ = instrument_model(curr_mel)\n",
    "            except:\n",
    "                inst_onset_pred, _, _, _, _ = instrument_model(curr_mel)\n",
    "            if verbose: print('inst onset pred shape', inst_onset_pred.shape)\n",
    "    if instrument_ckpt is not None:\n",
    "        if instrument_ckpt == 'D:/onsets-and-frames-master-multi/ckpts/transcriber-220315-040403-model-13.pt':\n",
    "            if USE_INSTRUMENT_GROUP:\n",
    "                # in this model, the last 5 * 88 entries are for: percussion, pluck, strings, wind, and general pitch\n",
    "                inst_onset_pred = inst_onset_pred[:, :, - 5 * N_KEYS:]\n",
    "            else:\n",
    "                # disregard the general instrument class but use the general pitch classes:\n",
    "                inst_onset_pred = torch.cat((inst_onset_pred[:, :, : - 5 * N_KEYS], inst_onset_pred[:, :, -N_KEYS:]), dim=-1)\n",
    "\n",
    "    if instrument_ckpt is not None:\n",
    "        # in the instrument models, the last 88 entries are for pitch classes. we can either use the pitch classes from the pitch model alone,\n",
    "        # or take the maximum activation between the two.\n",
    "\n",
    "        # onset_pred = torch.cat((inst_onset_pred[:, :, : -N_KEYS], pitch_onset_pred), dim=-1) # take pitch from the pitch model and instruments from the instrument model\n",
    "\n",
    "        # other option: take pitch to be maximum between pitch model and pitch classes of the instrument model (last N_KEYS=88 entries are pitch):\n",
    "        onset_pred = inst_onset_pred\n",
    "        onset_pred[:, :, -N_KEYS:] = torch.maximum(onset_pred[:, :, -N_KEYS:], pitch_onset_pred[:, :, :])\n",
    "\n",
    "        onset_pred = onset_pred.squeeze().cpu().numpy()\n",
    "        # choose maximum likelihood instrument for detected pitch classes:\n",
    "        onset_pred = max_inst(onset_pred, thr=onset_th, inactive=inactive)[:, : -N_KEYS]\n",
    "    else:\n",
    "        onset_pred = pitch_onset_pred.squeeze().cpu().numpy()\n",
    "\n",
    "    frames2midi(os.path.join(out_pth, \"midi\", pth.split('/')[-1].split(\".\")[0] + '.mid'), onset_pred, frame_pred.squeeze().cpu().numpy(), 64 * (onset_pred >= onset_th),\n",
    "                inst_mapping=[0] if instrument_ckpt is None else mappings[instrument_ckpt],\n",
    "                scaling=CURR_HOP_LENGTH / SAMPLE_RATE,\n",
    "                onset_threshold=onset_th, frame_threshold=frame_th)\n",
    "    \n",
    "    \n",
    "    np.savez(os.path.join(out_pth, \"raw_features\", pth.split('/')[-1].split(\".\")[0] + '.npz'),\n",
    "             onset_pred = onset_pred,\n",
    "             frame_pred = frame_pred.squeeze().cpu().numpy(),\n",
    "             sample_rate = SAMPLE_RATE,\n",
    "             hop_length = CURR_HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c9afd-113e-4178-9448-c81bdcd0a9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadfd62-4482-48fb-a76a-9dce3311b985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fae4e-bf72-4124-80b6-da4ada35184a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(2,1, figsize=(15,10))\n",
    "im0 = ax0.imshow(onset_pred.T, extent=[0, onset_pred.shape[0]/SAMPLE_RATE*CURR_HOP_LENGTH, 1, 88], origin='lower', aspect='auto', cmap='gray_r')\n",
    "plt.colorbar(im0, ax=ax0)\n",
    "ax0.grid()\n",
    "\n",
    "im1 = ax1.imshow(frame_pred.squeeze().cpu().numpy().T, extent=[0, onset_pred.shape[0]/SAMPLE_RATE*CURR_HOP_LENGTH, 1, 88], origin='lower', aspect='auto', cmap='gray_r')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "ax1.grid()\n",
    "\n",
    "lims = [0, 15]\n",
    "ax0.set_xlim(lims)\n",
    "ax1.set_xlim(lims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
